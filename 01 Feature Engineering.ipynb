{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os\n",
    "from datetime import datetime\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_s3_csv (dates):\n",
    "    s3 = boto3.resource('s3')\n",
    "    deutsche_boerse_bucket = 'deutsche-boerse-xetra-pds'\n",
    "    \n",
    "    bucket = s3.Bucket(deutsche_boerse_bucket)\n",
    "    \n",
    "    dataframes = []\n",
    "    \n",
    "    for date in dates:\n",
    "        csv_objects = bucket.objects.filter(Prefix=date)\n",
    "        for csv_obj in csv_objects:\n",
    "            csv_key = csv_obj.key\n",
    "            csv_body = csv_obj.get()['Body']\n",
    "            df = pd.read_csv(csv_body)\n",
    "            dataframes.append(df)\n",
    "    return pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earliest possible date is 2017-06-17\n",
    "from_date = '2017-07-01'\n",
    "until_date = '2017-07-31'\n",
    "\n",
    "dates = list(pd.date_range(from_date, until_date, freq='D').strftime('%Y-%m-%d'))\n",
    "\n",
    "unprocessed_df = read_s3_csv (dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (15, 10) # use bigger graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_df.info ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_df.Mnemonic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want the dates to be comparable to datetime.strptime()\n",
    "unprocessed_df[\"CalcTime\"] = pd.to_datetime(\"1900-01-01 \" + unprocessed_df[\"Time\"])\n",
    "unprocessed_df[\"CalcDateTime\"] = pd.to_datetime(unprocessed_df[\"Date\"] + \" \" + unprocessed_df[\"Time\"])\n",
    "unprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter common stock\n",
    "# Filter between trading hours 08:00 and 20:00\n",
    "# Exclude auctions (those are with TradeVolume == 0)\n",
    "only_common_stock = unprocessed_df[unprocessed_df.SecurityType == 'Common stock']\n",
    "time_fmt = \"%H:%M\"\n",
    "opening_hours_str = \"08:00\"\n",
    "closing_hours_str = \"20:00\"\n",
    "opening_hours = datetime.strptime(opening_hours_str, time_fmt)\n",
    "closing_hours = datetime.strptime(closing_hours_str, time_fmt)\n",
    "\n",
    "cleaned_common_stock = only_common_stock[(only_common_stock.TradedVolume > 0) & \\\n",
    "                  (only_common_stock.CalcTime >= opening_hours) & \\\n",
    "                  (only_common_stock.CalcTime <= closing_hours)]\n",
    "cleaned_common_stock.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bymnemonic = cleaned_common_stock[['Mnemonic', 'TradedVolume']].groupby(['Mnemonic']).sum()\n",
    "number_of_stocks = 100\n",
    "top = bymnemonic.sort_values(['TradedVolume'], ascending=[0]).head(number_of_stocks)\n",
    "top.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_stocks = list(top.index.values)\n",
    "cleaned_common_stock = cleaned_common_stock[cleaned_common_stock.Mnemonic.isin(top_k_stocks)]\n",
    "cleaned_common_stock.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_index = cleaned_common_stock.set_index(['Mnemonic', 'CalcDateTime']).sort_index()\n",
    "sorted_by_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_empty_days = sorted(list(cleaned_common_stock['Date'].unique()))\n",
    "len(non_empty_days), non_empty_days[0:2], '...', non_empty_days[-3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Ideal data count for any stock: {}\".format (44*12*60))\n",
    "print (\"Observation count per mnemonic:\")\n",
    "cleaned_common_stock.Mnemonic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def build_index(non_empty_days, from_time, to_time):\n",
    "    date_ranges = []\n",
    "    for date in non_empty_days:\n",
    "        yyyy, mm, dd = date.split('-')\n",
    "        from_hour, from_min = from_time.split(':')\n",
    "        to_hour, to_min = to_time.split(':')    \n",
    "        t1 = datetime.datetime(int(yyyy), int(mm), int(dd), int(from_hour),int(from_min),0)\n",
    "        t2 = datetime.datetime(int(yyyy), int(mm), int(dd), int(to_hour),int(to_min),0) \n",
    "        date_ranges.append(pd.DataFrame({\"OrganizedDateTime\": pd.date_range(t1, t2, freq='1Min').values}))\n",
    "    agg = pd.concat(date_ranges, axis=0) \n",
    "    agg.index = agg[\"OrganizedDateTime\"]\n",
    "    return agg\n",
    "new_datetime_index = build_index(non_empty_days, opening_hours_str, closing_hours_str)[\"OrganizedDateTime\"].values\n",
    "new_datetime_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_stock_features(input_df, mnemonic, new_time_index):\n",
    "    stock = sorted_by_index.loc[mnemonic].copy()\n",
    "    \n",
    "    stock = stock.reindex(new_time_index)\n",
    "    \n",
    "    features = ['MinPrice', 'MaxPrice', 'EndPrice', 'StartPrice']\n",
    "    for f in features:\n",
    "        stock[f] = stock[f].fillna(method='ffill')   \n",
    "    \n",
    "    features = ['TradedVolume', 'NumberOfTrades']\n",
    "    for f in features:\n",
    "        stock[f] = stock[f].fillna(0.0)\n",
    "        \n",
    "    stock['HourOfDay'] = stock.index.hour\n",
    "    stock['MinOfHour'] = stock.index.minute\n",
    "    stock['MinOfDay'] = stock.index.hour*60 + stock.index.minute\n",
    "\n",
    "    stock['DayOfWeek'] = stock.index.dayofweek\n",
    "    stock['DayOfYear'] = stock.index.dayofyear\n",
    "    stock['MonthOfYear'] = stock.index.month\n",
    "    stock['WeekOfYear'] = stock.index.weekofyear\n",
    "    \n",
    "    stock['Mnemonic'] = mnemonic\n",
    "    unwanted_features = ['ISIN', 'SecurityDesc', 'SecurityType', 'Currency', 'SecurityID', 'Date', 'Time', 'CalcTime']\n",
    "    return stock.drop (unwanted_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sorted_by_index.loc['DAI'].copy ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.index.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = []\n",
    "for stock in top_k_stocks:\n",
    "    stock = basic_stock_features(sorted_by_index, stock, new_datetime_index)\n",
    "    stocks.append(stock)\n",
    "# prepared should contain the numeric features for all top k stocks,\n",
    "# for all days in the interval, for which there were trades (that means excluding weekends and holidays)\n",
    "# for all minutes from 08:00 until 20:00\n",
    "# in minutes without trades the prices from the last available minute are carried forward\n",
    "# trades are filled with zero for such minutes\n",
    "# a new column called HasTrade is introduced to denote the presence of trades\n",
    "prepared = pd.concat(stocks, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared.Mnemonic = prepared.Mnemonic.astype('category')\n",
    "prepared.Mnemonic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Convert timestamp to more meaningful derived features\n",
    "\n",
    "**TODO** Integrate with plotly for histograms, correlation matrices, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_index.loc['DAI'].tail ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save both in csv in pickle. Generally we'd read from the pickeled format because \n",
    "# it preserves the indices, but for cases where pkl cannot be read, we also output a csv format\n",
    "output_folder = 'data/processed' # do not end in /\n",
    "! mkdir -p {output_folder}\n",
    "\n",
    "prepared.to_csv(output_folder + '/cooked_v3.csv')\n",
    "\n",
    "prepared.to_pickle(output_folder + '/cooked_v3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {output_folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
