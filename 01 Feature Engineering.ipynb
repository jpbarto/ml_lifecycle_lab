{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earliest possible date is 2017-06-17\n",
    "from_date = '2017-07-01'\n",
    "until_date = '2017-07-31'\n",
    "\n",
    "local_data_folder = 'data/raw' # do not end in /\n",
    "output_folder = 'data/processed' # do not end in /\n",
    "\n",
    "download_script = './download_data.sh'\n",
    "\n",
    "dates = list(pd.date_range(from_date, until_date, freq='D').strftime('%Y-%m-%d'))\n",
    "\n",
    "! mkdir -p {local_data_folder}\n",
    "\n",
    "# We found it was more reliable to generate a bash script and run it, rather than\n",
    "# run the commands in a python for-loop\n",
    "\n",
    "with open(download_script, 'w') as f:\n",
    "    f.write(\"#!/bin/bash\\n\")\n",
    "    f.write(\"\\nset -euo pipefail\\n\")\n",
    "    f.write(\"\\n# This script was generated to download data for multiple days\\n\")\n",
    "    for date in dates:\n",
    "        success_file =  os.path.join(local_data_folder, date, 'success')\n",
    "\n",
    "        f.write(\"\"\"\n",
    "if [ ! -f {success_file} ]; then\n",
    "\n",
    "    echo \"Getting PDS dataset for date {date}\"        \n",
    "    mkdir -p {local_data_folder}/{date}\n",
    "    aws s3 sync s3://deutsche-boerse-xetra-pds/{date} {local_data_folder}/{date} --no-sign-request\n",
    "    touch {success_file}            \n",
    "else\n",
    "    echo \"PDS dataset for date {date} already exists\"\n",
    "fi\\n\"\"\".format(success_file=success_file, date=date, local_data_folder=local_data_folder))\n",
    "\n",
    "        \n",
    "! chmod +x {download_script}     \n",
    "! head -n 15 {download_script} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute the download script to retrieve the data\n",
    "!  {download_script}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os\n",
    "from datetime import datetime\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (15, 10) # use bigger graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_dirs(data_dirs):\n",
    "    files = []\n",
    "    for data_dir in data_dirs:\n",
    "        files.extend(glob.glob(os.path.join(data_dir, '*.csv')))\n",
    "    return pd.concat(map(pd.read_csv, files))\n",
    "\n",
    "data_dir = local_data_folder + '/'\n",
    "data_subdirs = map(lambda date: data_dir + date, dates)\n",
    "unprocessed_df = load_csv_dirs(data_subdirs)\n",
    "unprocessed_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_df.info ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_df.Mnemonic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want the dates to be comparable to datetime.strptime()\n",
    "unprocessed_df[\"CalcTime\"] = pd.to_datetime(\"1900-01-01 \" + unprocessed_df[\"Time\"])\n",
    "unprocessed_df[\"CalcDateTime\"] = pd.to_datetime(unprocessed_df[\"Date\"] + \" \" + unprocessed_df[\"Time\"])\n",
    "unprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter common stock\n",
    "# Filter between trading hours 08:00 and 20:00\n",
    "# Exclude auctions (those are with TradeVolume == 0)\n",
    "only_common_stock = unprocessed_df[unprocessed_df.SecurityType == 'Common stock']\n",
    "time_fmt = \"%H:%M\"\n",
    "opening_hours_str = \"08:00\"\n",
    "closing_hours_str = \"20:00\"\n",
    "opening_hours = datetime.strptime(opening_hours_str, time_fmt)\n",
    "closing_hours = datetime.strptime(closing_hours_str, time_fmt)\n",
    "\n",
    "cleaned_common_stock = only_common_stock[(only_common_stock.TradedVolume > 0) & \\\n",
    "                  (only_common_stock.CalcTime >= opening_hours) & \\\n",
    "                  (only_common_stock.CalcTime <= closing_hours)]\n",
    "cleaned_common_stock.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bymnemonic = cleaned_common_stock[['Mnemonic', 'TradedVolume']].groupby(['Mnemonic']).sum()\n",
    "number_of_stocks = 100\n",
    "top = bymnemonic.sort_values(['TradedVolume'], ascending=[0]).head(number_of_stocks)\n",
    "top.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_stocks = list(top.index.values)\n",
    "cleaned_common_stock = cleaned_common_stock[cleaned_common_stock.Mnemonic.isin(top_k_stocks)]\n",
    "cleaned_common_stock.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_index = cleaned_common_stock.set_index(['Mnemonic', 'CalcDateTime']).sort_index()\n",
    "sorted_by_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_empty_days = sorted(list(cleaned_common_stock['Date'].unique()))\n",
    "len(non_empty_days), non_empty_days[0:2], '...', non_empty_days[-3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Ideal data count for any stock: {}\".format (44*12*60))\n",
    "print (\"Observation count per mnemonic:\")\n",
    "cleaned_common_stock.Mnemonic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def build_index(non_empty_days, from_time, to_time):\n",
    "    date_ranges = []\n",
    "    for date in non_empty_days:\n",
    "        yyyy, mm, dd = date.split('-')\n",
    "        from_hour, from_min = from_time.split(':')\n",
    "        to_hour, to_min = to_time.split(':')    \n",
    "        t1 = datetime.datetime(int(yyyy), int(mm), int(dd), int(from_hour),int(from_min),0)\n",
    "        t2 = datetime.datetime(int(yyyy), int(mm), int(dd), int(to_hour),int(to_min),0) \n",
    "        date_ranges.append(pd.DataFrame({\"OrganizedDateTime\": pd.date_range(t1, t2, freq='1Min').values}))\n",
    "    agg = pd.concat(date_ranges, axis=0) \n",
    "    agg.index = agg[\"OrganizedDateTime\"]\n",
    "    return agg\n",
    "new_datetime_index = build_index(non_empty_days, opening_hours_str, closing_hours_str)[\"OrganizedDateTime\"].values\n",
    "new_datetime_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_stock_features(input_df, mnemonic, new_time_index):\n",
    "    stock = sorted_by_index.loc[mnemonic].copy()\n",
    "    \n",
    "    stock = stock.reindex(new_time_index)\n",
    "    \n",
    "    features = ['MinPrice', 'MaxPrice', 'EndPrice', 'StartPrice']\n",
    "    for f in features:\n",
    "        stock[f] = stock[f].fillna(method='ffill')   \n",
    "    \n",
    "    features = ['TradedVolume', 'NumberOfTrades']\n",
    "    for f in features:\n",
    "        stock[f] = stock[f].fillna(0.0)\n",
    "        \n",
    "    stock['HourOfDay'] = stock.index.hour\n",
    "    stock['MinOfHour'] = stock.index.minute\n",
    "    stock['MinOfDay'] = stock.index.hour*60 + stock.index.minute\n",
    "\n",
    "    stock['DayOfWeek'] = stock.index.dayofweek\n",
    "    stock['DayOfYear'] = stock.index.dayofyear\n",
    "    stock['MonthOfYear'] = stock.index.month\n",
    "    stock['WeekOfYear'] = stock.index.weekofyear\n",
    "    \n",
    "    stock['Mnemonic'] = mnemonic\n",
    "    unwanted_features = ['ISIN', 'SecurityDesc', 'SecurityType', 'Currency', 'SecurityID', 'Date', 'Time', 'CalcTime']\n",
    "    return stock.drop (unwanted_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sorted_by_index.loc['DAI'].copy ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.index.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = []\n",
    "for stock in top_k_stocks:\n",
    "    stock = basic_stock_features(sorted_by_index, stock, new_datetime_index)\n",
    "    stocks.append(stock)\n",
    "# prepared should contain the numeric features for all top k stocks,\n",
    "# for all days in the interval, for which there were trades (that means excluding weekends and holidays)\n",
    "# for all minutes from 08:00 until 20:00\n",
    "# in minutes without trades the prices from the last available minute are carried forward\n",
    "# trades are filled with zero for such minutes\n",
    "# a new column called HasTrade is introduced to denote the presence of trades\n",
    "prepared = pd.concat(stocks, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared.Mnemonic = prepared.Mnemonic.astype('category')\n",
    "prepared.Mnemonic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Convert timestamp to more meaningful derived features\n",
    "\n",
    "**TODO** Integrate with plotly for histograms, correlation matrices, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_index.loc['DAI'].tail ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save both in csv in pickle. Generally we'd read from the pickeled format because \n",
    "# it preserves the indices, but for cases where pkl cannot be read, we also output a csv format\n",
    "! mkdir -p {output_folder}\n",
    "prepared.to_csv(output_folder + '/cooked_v3.csv')\n",
    "\n",
    "prepared.to_pickle(output_folder + '/cooked_v3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {output_folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
