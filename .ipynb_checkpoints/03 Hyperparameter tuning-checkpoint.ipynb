{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "This lab addresses two challenges of the machine learning lifecycle.  The first is making the logic developed in labs 1 and 2 more formalized and reusable by wrapping it in Python functions.  The second is determining the ideal hyperparameters for a selected algorithm in order to optimize model performance and accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## Feature engineering re-use\n",
    "\n",
    "Raw data needs to be converted to features both during the training stage of a model's life but also during production use of the model.  In the same way that raw data is transformed before being sent with an algorithm to training, raw data must also be transformed before it can be used to make predictions at run time.  Therefore its important to be able to make the feature engineering logic reusable and portable.  This can be done as a RESTful microservice or a Python code module, any method the delivery team prefers.  Today we will stop at formalizing the logic as a collection of functions.  But bear in mind that this logic would need to be re-used during runtime as well as at training.\n",
    "\n",
    "Review the code below, its an almagamation of the code from the previous two notebooks.  Run the cells to create a data set for use with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**S3 bucket for training data**\n",
    "\n",
    "The next cell defines a variable used to store training and validation data for hyperparameter optimization and training.  Specify a bucket name below and the S3 bucket will be created on your behalf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_BUCKET_NAME = ' < YOUR S3 BUCKET NAME > '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'hpo'\n",
    "s3_hpo_uri = 's3://{}/{}/'.format (YOUR_BUCKET_NAME, prefix)\n",
    "\n",
    "!aws s3 mb \"s3://{YOUR_BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_s3_csv (dates):\n",
    "    s3 = boto3.resource('s3')\n",
    "    deutsche_boerse_bucket = 'deutsche-boerse-xetra-pds'\n",
    "    \n",
    "    bucket = s3.Bucket(deutsche_boerse_bucket)\n",
    "    \n",
    "    dataframes = []\n",
    "    \n",
    "    for date in dates:\n",
    "        objs_count = 0\n",
    "        csv_objects = bucket.objects.filter(Prefix=date)\n",
    "        for csv_obj in csv_objects:\n",
    "            csv_key = csv_obj.key\n",
    "            if csv_key[-4:] == '.csv':\n",
    "                objs_count += 1\n",
    "                csv_body = csv_obj.get()['Body']\n",
    "                df = pd.read_csv(csv_body)\n",
    "                dataframes.append(df)\n",
    "        \n",
    "        print (\"Loaded {} data objects for {}\".format (objs_count, date))\n",
    "    return pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(non_empty_days, from_time, to_time):\n",
    "    date_ranges = []\n",
    "    for date in non_empty_days:\n",
    "        yyyy, mm, dd = date.split('-')\n",
    "        from_hour, from_min = from_time.split(':')\n",
    "        to_hour, to_min = to_time.split(':')    \n",
    "        t1 = datetime(int(yyyy), int(mm), int(dd), int(from_hour),int(from_min),0)\n",
    "        t2 = datetime(int(yyyy), int(mm), int(dd), int(to_hour),int(to_min),0) \n",
    "        date_ranges.append(pd.DataFrame({\"OrganizedDateTime\": pd.date_range(t1, t2, freq='1Min').values}))\n",
    "    agg = pd.concat(date_ranges, axis=0) \n",
    "    agg.index = agg[\"OrganizedDateTime\"]\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_stock_features(input_df, mnemonic, new_time_index, inplace=False):\n",
    "    stock = input_df.loc[mnemonic]\n",
    "    if not inplace:\n",
    "        stock = input_df.loc[mnemonic].copy()\n",
    "    \n",
    "    stock = stock.reindex(new_time_index)\n",
    "    \n",
    "    features = ['MinPrice', 'MaxPrice', 'EndPrice', 'StartPrice']\n",
    "    for f in features:\n",
    "        stock[f] = stock[f].fillna(method='ffill')   \n",
    "    \n",
    "    features = ['TradedVolume', 'NumberOfTrades']\n",
    "    for f in features:\n",
    "        stock[f] = stock[f].fillna(0.0)\n",
    "        \n",
    "    stock['HourOfDay'] = stock.index.hour\n",
    "    stock['MinOfHour'] = stock.index.minute\n",
    "    stock['MinOfDay'] = stock.index.hour*60 + stock.index.minute\n",
    "\n",
    "    stock['DayOfWeek'] = stock.index.dayofweek\n",
    "    stock['DayOfYear'] = stock.index.dayofyear\n",
    "    stock['MonthOfYear'] = stock.index.month\n",
    "    stock['WeekOfYear'] = stock.index.weekofyear\n",
    "    \n",
    "    stock['Mnemonic'] = mnemonic\n",
    "    unwanted_features = ['ISIN', 'SecurityDesc', 'SecurityType', 'Currency', 'SecurityID', 'Date', 'Time', 'CalcTime']\n",
    "    return stock.drop (unwanted_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data (df, inplace = False):\n",
    "    column_filter = ['ISIN', 'Mnemonic', 'SecurityDesc', 'SecurityType', 'Currency', 'SecurityID', 'Date', 'Time', 'StartPrice', 'MaxPrice', 'MinPrice', 'EndPrice', 'TradedVolume', 'NumberOfTrades']\n",
    "    n_df = df[column_filter]\n",
    "    if not inplace:\n",
    "        n_df = df.copy ()\n",
    "        \n",
    "    n_df.drop (n_df.Time == 'Time', inplace = True)\n",
    "    # we want the dates to be comparable to datetime.strptime()\n",
    "    n_df[\"CalcTime\"] = pd.to_datetime(\"1900-01-01 \" + n_df[\"Time\"], errors='coerce')\n",
    "    n_df[\"CalcDateTime\"] = pd.to_datetime(n_df[\"Date\"] + \" \" + n_df[\"Time\"], errors='coerce')\n",
    "\n",
    "    # Filter common stock\n",
    "    # Filter between trading hours 08:00 and 20:00\n",
    "    # Exclude auctions (those are with TradeVolume == 0)\n",
    "    only_common_stock = n_df[n_df.SecurityType == 'Common stock']\n",
    "    time_fmt = \"%H:%M\"\n",
    "    opening_hours_str = \"08:00\"\n",
    "    closing_hours_str = \"20:00\"\n",
    "    opening_hours = datetime.strptime(opening_hours_str, time_fmt)\n",
    "    closing_hours = datetime.strptime(closing_hours_str, time_fmt)\n",
    "\n",
    "    cleaned_common_stock = only_common_stock[(only_common_stock.TradedVolume > 0) & \\\n",
    "                      (only_common_stock.CalcTime >= opening_hours) & \\\n",
    "                      (only_common_stock.CalcTime <= closing_hours)]\n",
    "    \n",
    "    bymnemonic = cleaned_common_stock[['Mnemonic', 'TradedVolume']].groupby(['Mnemonic']).sum()\n",
    "    number_of_stocks = 100\n",
    "    top = bymnemonic.sort_values(['TradedVolume'], ascending=[0]).head(number_of_stocks)\n",
    "    top_k_stocks = list(top.index.values)\n",
    "    cleaned_common_stock = cleaned_common_stock[cleaned_common_stock.Mnemonic.isin(top_k_stocks)]\n",
    "    sorted_by_index = cleaned_common_stock.set_index(['Mnemonic', 'CalcDateTime']).sort_index()\n",
    "    non_empty_days = sorted(list(cleaned_common_stock['Date'].unique()))\n",
    "    new_datetime_index = build_index(non_empty_days, opening_hours_str, closing_hours_str)[\"OrganizedDateTime\"].values\n",
    "    \n",
    "    stocks = []\n",
    "    for stock in top_k_stocks:\n",
    "        stock = basic_stock_features(sorted_by_index, stock, new_datetime_index, inplace=True)\n",
    "        stocks.append(stock)\n",
    "    # prepared should contain the numeric features for all top k stocks,\n",
    "    # for all days in the interval, for which there were trades (that means excluding weekends and holidays)\n",
    "    # for all minutes from 08:00 until 20:00\n",
    "    # in minutes without trades the prices from the last available minute are carried forward\n",
    "    # trades are filled with zero for such minutes\n",
    "    # a new column called HasTrade is introduced to denote the presence of trades\n",
    "    prepared = pd.concat(stocks, axis=0).dropna(how='any')\n",
    "    prepared.Mnemonic = prepared.Mnemonic.astype('category')\n",
    "    return prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xgb_target (df):\n",
    "    return df.MaxPrice.shift(-1).fillna (method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xgb_features (df, horizon, inplace = False):\n",
    "    n_df = df\n",
    "    if not inplace:\n",
    "        n_df = df.copy ()\n",
    "    \n",
    "    for offset in range(1, horizon+1):\n",
    "        min_price = n_df['MinPrice'].shift (offset).fillna(method='bfill')\n",
    "        max_price = n_df['MaxPrice'].shift (offset).fillna(method='bfill')\n",
    "        start_price = n_df['StartPrice'].shift (offset).fillna(method='bfill')\n",
    "        end_price = n_df['EndPrice'].shift (offset).fillna(method='bfill')\n",
    "        trade_vol = n_df['TradedVolume'].shift (offset).fillna(method='bfill')\n",
    "        num_trades = n_df['NumberOfTrades'].shift (offset).fillna(method='bfill')\n",
    "        \n",
    "        n_df[\"h{}_MinPrice\".format (offset)] = min_price\n",
    "        n_df[\"h{}_MaxPrice\".format (offset)] = max_price\n",
    "        n_df[\"h{}_StartPrice\".format (offset)] = start_price\n",
    "        n_df[\"h{}_EndPrice\".format (offset)] = end_price\n",
    "        n_df[\"h{}_TradeVolume\".format (offset)] = trade_vol\n",
    "        n_df[\"h{}_NumberOfTrades\".format (offset)] = num_trades\n",
    "        \n",
    "    return n_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_date_range (dates):\n",
    "    unprocessed_df = read_s3_csv (dates)\n",
    "    print (\"Loaded CSV data set from S3\")\n",
    "    \n",
    "    cleaned_df = clean_data (unprocessed_df, inplace = True)\n",
    "    print (\"Cleaned CSV data set\")\n",
    "     \n",
    "    xgb_data = create_xgb_features (cleaned_df, 5, inplace=True)\n",
    "    xgb_data['NextMaxPrice'] = create_xgb_target (xgb_data)\n",
    "    print (\"Engineered CSV data set\")\n",
    "    \n",
    "    train_data, validate_data = train_test_split (xgb_data, train_size=0.8, test_size=0.2, shuffle=True)\n",
    "\n",
    "    cols = list(train_data.columns.values)\n",
    "    cols.remove ('NextMaxPrice')\n",
    "    cols = ['NextMaxPrice'] + cols\n",
    "\n",
    "    train_data = pd.get_dummies (train_data[cols])\n",
    "    validate_data = pd.get_dummies (validate_data[cols])\n",
    "    print (\"Data split for training purposes\")\n",
    "    \n",
    "    return (train_data, validate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_data_folder = 'data/hpo'\n",
    "train_output_folder = hpo_data_folder +'/train'\n",
    "validate_output_folder = hpo_data_folder +'/validate'\n",
    "! mkdir -p {train_output_folder}\n",
    "! mkdir -p {validate_output_folder}\n",
    "\n",
    "# Earliest possible date is 2017-06-17\n",
    "from_date = '2017-09-01'\n",
    "until_date = '2017-11-30'\n",
    "dates = list(pd.date_range(from_date, until_date, freq='D').strftime('%Y-%m-%d'))\n",
    "\n",
    "print (\"Reading data for dates {} to {}\".format (from_date, until_date))\n",
    "train_df, validate_df = engineer_date_range (dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Writing CSV data for dates {} to {}\".format (from_date, until_date))\n",
    "\n",
    "chunk_size = 100000\n",
    "print (\"Writing {} training records\".format (train_df.shape[0]))\n",
    "for chunk_index in range (0, train_df.shape[0], chunk_size):\n",
    "    train_df[chunk_index:chunk_index + chunk_size].to_csv(train_output_folder + '/{}-{}_{}-{}.csv'.format (from_date, until_date, chunk_index, chunk_index + chunk_size), header=False, index=False)\n",
    "    print (\"Wrote training records {} to {}\".format (chunk_index, chunk_index + chunk_size))\n",
    "\n",
    "print (\"Writing {} validation records\".format (validate_df.shape[0]))\n",
    "for chunk_index in range (0, validate_df.shape[0], chunk_size):\n",
    "    validate_df[chunk_index:chunk_index + chunk_size].to_csv(validate_output_folder + '/{}-{}_{}-{}.csv'.format (from_date, until_date, chunk_index, chunk_index + chunk_size), header=False, index=False)\n",
    "    print (\"Wrote validation records {} to {}\".format (chunk_index, chunk_index + chunk_size))\n",
    "print (\"Export to CSV complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync {hpo_data_folder} {s3_hpo_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "---\n",
    "\n",
    "During the previous lab we ran spot checks against multiple machine learning algorithms.  XGBoost looked like it was worth pursuing and so we will create a hyperparameter tuning job on a subset of the overall data set in order to determine the most effective hyperparameters for the algorithm.  Using the data generated above, create a hyperparameter tuning job to allow Amazon SageMaker to search the settings for an optimal collection.  In the next lab you will use these settings to train XGBoost on the entire data set.\n",
    "\n",
    "### Using the AWS Console\n",
    "1. Upload the train and validate data sets to an S3 bucket in your preferred region with SageMaker available\n",
    "1. From the SageMaker console click `Hyperparameter tuning jobs` --> `Create hyperparameter tuning job`\n",
    "1. Give the tuning job a name, such as 'XGBoost-forecast-01'\n",
    "1. Select `XGBoost` from the drop down of Amazon SageMaker built-in algorithms, click `Next`\n",
    "1. For Objective metric select `validation:rmse`\n",
    "1. Set the following hyperparameters as:\n",
    "  1. num_round from 10 to 100\n",
    "  1. eta from 0.2 to 0.9\n",
    "  1. gamma from 0.1 to 9.0\n",
    "  1. max_depth from 3 to 10\n",
    "1. Click `Next`\n",
    "1. Define two training channels pointing to the train and the validate data sets pushed earlier to S3\n",
    "1. For the train data set specify\n",
    "  1. `Channel name` to `train`\n",
    "  1. `Content type` to `csv`\n",
    "  1. `Compression type` and `Record wrapper` to `None`\n",
    "  1. `S3 Data Type` to `S3Prefix`\n",
    "  1. `S3 Data Distribution` to `Fully replicated`\n",
    "  1. Specify the `URI` as the S3 train folder in `s3_hpo_uri`\n",
    "  1. Set `Input mode` to `File`\n",
    "1. Click `Add channel` and specify the validate data set\n",
    "  1. `Channel name` to `validation`\n",
    "  1. `Content type` to `csv`\n",
    "  1. `Compression type` and `Record wrapper` to `None`\n",
    "  1. `S3 Data Type` to `S3Prefix`\n",
    "  1. `S3 Data Distribution` to `Fully replicated`\n",
    "  1. Specify the `URI` as the S3 validate folder in `s3_hpo_uri`\n",
    "  1. Set `Input mode` to `File`\n",
    "1. Specify the `S3 output path` and click `Next`\n",
    "1. Specify an instance type of `m4.4xlarge`\n",
    "1. Specify 20 GB of storage\n",
    "1. Specify a maximum training job count of 6 and a maximum of 2 parallel training jobs\n",
    "1. Click `Create jobs`\n",
    "\n",
    "**Note:** Execution of this HPO job will take approximately 20 minutes.\n",
    "\n",
    "### Using the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_config = {\n",
    "    \"ParameterRanges\": {\n",
    "      \"CategoricalParameterRanges\": [],\n",
    "      \"ContinuousParameterRanges\": [\n",
    "        {\n",
    "          \"MaxValue\": \"0.9\",\n",
    "          \"MinValue\": \"0.1\",\n",
    "          \"Name\": \"eta\"\n",
    "        },\n",
    "        {\n",
    "          \"MaxValue\": \"2\",\n",
    "          \"MinValue\": \"0\",\n",
    "          \"Name\": \"alpha\"\n",
    "        },\n",
    "        {\n",
    "          \"MaxValue\": \"9.0\",\n",
    "          \"MinValue\": \"0.1\",\n",
    "          \"Name\": \"gamma\"\n",
    "        },\n",
    "        {\n",
    "          \"MaxValue\": \"10\",\n",
    "          \"MinValue\": \"1\",\n",
    "          \"Name\": \"min_child_weight\"\n",
    "        }\n",
    "      ],\n",
    "      \"IntegerParameterRanges\": [\n",
    "        {\n",
    "          \"MaxValue\": \"10\",\n",
    "          \"MinValue\": \"3\",\n",
    "          \"Name\": \"max_depth\"\n",
    "        },\n",
    "        {\n",
    "          \"MaxValue\": \"100\",\n",
    "          \"MinValue\": \"10\",\n",
    "          \"Name\": \"num_round\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"ResourceLimits\": {\n",
    "      \"MaxNumberOfTrainingJobs\": 10,\n",
    "      \"MaxParallelTrainingJobs\": 3\n",
    "    },\n",
    "    \"Strategy\": \"Bayesian\",\n",
    "    \"HyperParameterTuningJobObjective\": {\n",
    "      \"MetricName\": \"validation:rmse\",\n",
    "      \"Type\": \"Minimize\"\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "training_image = get_image_uri(boto3.Session().region_name, 'xgboost')\n",
    "\n",
    "s3_input_train = 's3://{}/{}/train'.format(YOUR_BUCKET_NAME, prefix)\n",
    "s3_input_validation ='s3://{}/{}/validate/'.format(YOUR_BUCKET_NAME, prefix)\n",
    "     \n",
    "training_job_definition = {\n",
    "    \"AlgorithmSpecification\": {\n",
    "      \"TrainingImage\": training_image,\n",
    "      \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "      {\n",
    "        \"ChannelName\": \"train\",\n",
    "        \"CompressionType\": \"None\",\n",
    "        \"ContentType\": \"csv\",\n",
    "        \"DataSource\": {\n",
    "          \"S3DataSource\": {\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": s3_input_train\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"ChannelName\": \"validation\",\n",
    "        \"CompressionType\": \"None\",\n",
    "        \"ContentType\": \"csv\",\n",
    "        \"DataSource\": {\n",
    "          \"S3DataSource\": {\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": s3_input_validation\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ],\n",
    "    \"OutputDataConfig\": {\n",
    "      \"S3OutputPath\": \"s3://{}/{}/output\".format(YOUR_BUCKET_NAME, prefix)\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "      \"InstanceCount\": 1,\n",
    "      \"InstanceType\": \"ml.c5.4xlarge\",\n",
    "      \"VolumeSizeInGB\": 20\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"StaticHyperParameters\": {\n",
    "      \"eval_metric\": \"rmse\",\n",
    "      \"objective\": \"reg:linear\",\n",
    "      \"rate_drop\": \"0.3\",\n",
    "      \"tweedie_variance_power\": \"1.4\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "      \"MaxRuntimeInSeconds\": 43200\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tuning_job_name = \"forecast-tuning-{}\".format (datetime.now ().strftime(\"%d%H%M\"))\n",
    "\n",
    "smclient = boto3.client ('sagemaker')\n",
    "smclient.create_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name,\n",
    "                                           HyperParameterTuningJobConfig = tuning_job_config,\n",
    "                                           TrainingJobDefinition = training_job_definition)\n",
    "status = smclient.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)['HyperParameterTuningJobStatus']\n",
    "print(status)\n",
    "while status !='Completed' and status!='Failed':\n",
    "    time.sleep(60)\n",
    "    status = smclient.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)['HyperParameterTuningJobStatus']\n",
    "    print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note optimal hyperparameter values\n",
    "\n",
    "---\n",
    "\n",
    "Once the hyperparameter tuning job has completed make a note of the hyperparameters that produced the best performing model.  You will need to specify these values in the next lab when training on the full data set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
