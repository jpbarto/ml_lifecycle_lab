{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model\n",
    "\n",
    "---\n",
    "\n",
    "We have now formalized our feature engineering logic and have a good idea of the optimal hyperparameters for the XGBoost algorithm.  During hyperparameter tuning we were still using a subset of the data in order to speed up our training times.  Now its time to train on the entire data set and produce a working model.  The steps below are an execution of the feature engineering logic on a year's worth of data.  This takes a long time to process and I would recommend running `aws s3 sync` on `s3://jasbarto-forecast-lab/train` to skip having to run this yourself.\n",
    "\n",
    "After you have the data set on the local notebook instance upload it to S3 for training on Amazon SageMaker.\n",
    "\n",
    "## Prepare the entire data set for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earliest possible date is 2017-06-17\n",
    "from_date = '2017-08-01'\n",
    "until_date = '2018-07-31'\n",
    "\n",
    "dates = list(pd.date_range(from_date, until_date, freq='D').strftime('%Y-%m-%d'))\n",
    "\n",
    "local_data_folder = 'data/raw' # do not end in /\n",
    "! mkdir -p {local_data_folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_script = './download_data.sh'\n",
    "\n",
    "# We found it was more reliable to generate a bash script and run it, rather than\n",
    "# run the commands in a python for-loop\n",
    "\n",
    "with open(download_script, 'w') as f:\n",
    "    f.write(\"#!/bin/bash\\n\")\n",
    "    f.write(\"\\nset -euo pipefail\\n\")\n",
    "    f.write(\"\\n# This script was generated to download data for multiple days\\n\")\n",
    "    for date in dates:\n",
    "        success_file =  os.path.join(local_data_folder, date, 'success')\n",
    "\n",
    "        f.write(\"\"\"\n",
    "if [ ! -f {success_file} ]; then\n",
    "\n",
    "    echo \"Getting PDS dataset for date {date}\"        \n",
    "    mkdir -p {local_data_folder}/{date}\n",
    "    aws s3 sync s3://deutsche-boerse-xetra-pds/{date} {local_data_folder}/{date} --no-sign-request\n",
    "    touch {success_file}            \n",
    "else\n",
    "    echo \"PDS dataset for date {date} already exists\"\n",
    "fi\\n\"\"\".format(success_file=success_file, date=date, local_data_folder=local_data_folder))\n",
    "\n",
    "        \n",
    "! chmod +x {download_script}     \n",
    "! head -n 15 {download_script} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute the download script to retrieve the data\n",
    "!  {download_script}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_dirs(data_dirs):\n",
    "    files = []\n",
    "    for data_dir in data_dirs:\n",
    "        files.extend(glob.glob(os.path.join(data_dir, '*.csv')))\n",
    "    return pd.concat(map(pd.read_csv, files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(non_empty_days, from_time, to_time):\n",
    "    date_ranges = []\n",
    "    for date in non_empty_days:\n",
    "        yyyy, mm, dd = date.split('-')\n",
    "        from_hour, from_min = from_time.split(':')\n",
    "        to_hour, to_min = to_time.split(':')    \n",
    "        t1 = datetime(int(yyyy), int(mm), int(dd), int(from_hour),int(from_min),0)\n",
    "        t2 = datetime(int(yyyy), int(mm), int(dd), int(to_hour),int(to_min),0) \n",
    "        date_ranges.append(pd.DataFrame({\"OrganizedDateTime\": pd.date_range(t1, t2, freq='1Min').values}))\n",
    "    agg = pd.concat(date_ranges, axis=0) \n",
    "    agg.index = agg[\"OrganizedDateTime\"]\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_stock_features(input_df, mnemonic, new_time_index, inplace=False):\n",
    "    stock = input_df.loc[mnemonic]\n",
    "    if not inplace:\n",
    "        stock = input_df.loc[mnemonic].copy()\n",
    "    \n",
    "    stock = stock.reindex(new_time_index)\n",
    "    \n",
    "    features = ['MinPrice', 'MaxPrice', 'EndPrice', 'StartPrice']\n",
    "    for f in features:\n",
    "        stock[f] = stock[f].fillna(method='ffill')   \n",
    "    \n",
    "    features = ['TradedVolume', 'NumberOfTrades']\n",
    "    for f in features:\n",
    "        stock[f] = stock[f].fillna(0.0)\n",
    "        \n",
    "    stock['HourOfDay'] = stock.index.hour\n",
    "    stock['MinOfHour'] = stock.index.minute\n",
    "    stock['MinOfDay'] = stock.index.hour*60 + stock.index.minute\n",
    "\n",
    "    stock['DayOfWeek'] = stock.index.dayofweek\n",
    "    stock['DayOfYear'] = stock.index.dayofyear\n",
    "    stock['MonthOfYear'] = stock.index.month\n",
    "    stock['WeekOfYear'] = stock.index.weekofyear\n",
    "    \n",
    "    stock['Mnemonic'] = mnemonic\n",
    "    unwanted_features = ['ISIN', 'SecurityDesc', 'SecurityType', 'Currency', 'SecurityID', 'Date', 'Time', 'CalcTime']\n",
    "    return stock.drop (unwanted_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "error_df = None\n",
    "def clean_data (df, inplace = False):\n",
    "    global error_df\n",
    "    n_df = df\n",
    "    if not inplace:\n",
    "        n_df = df.copy ()\n",
    "        \n",
    "    n_df.drop (n_df.Time == 'Time', inplace = True) # some records have headers for values, remove them\n",
    "    # we want the dates to be comparable to datetime.strptime()\n",
    "    try:\n",
    "        n_df[\"CalcTime\"] = pd.to_datetime(\"1900-01-01 \" + n_df[\"Time\"], errors='coerce')\n",
    "        n_df[\"CalcDateTime\"] = pd.to_datetime(n_df[\"Date\"] + \" \" + n_df[\"Time\"], errors='coerce')\n",
    "    except:\n",
    "        print (\"Error processing date / time fields in dataframe:\")\n",
    "        print (sys.exc_info ())\n",
    "        tb = sys.exc_info()[2]\n",
    "        print (n_df.sample(10))\n",
    "        error_df = n_df\n",
    "        return pd.DataFrame ()\n",
    "        \n",
    "\n",
    "    # Filter common stock\n",
    "    # Filter between trading hours 08:00 and 20:00\n",
    "    # Exclude auctions (those are with TradeVolume == 0)\n",
    "    only_common_stock = n_df[n_df.SecurityType == 'Common stock']\n",
    "    time_fmt = \"%H:%M\"\n",
    "    opening_hours_str = \"08:00\"\n",
    "    closing_hours_str = \"20:00\"\n",
    "    opening_hours = datetime.strptime(opening_hours_str, time_fmt)\n",
    "    closing_hours = datetime.strptime(closing_hours_str, time_fmt)\n",
    "\n",
    "    cleaned_common_stock = only_common_stock[(only_common_stock.TradedVolume > 0) & \\\n",
    "                      (only_common_stock.CalcTime >= opening_hours) & \\\n",
    "                      (only_common_stock.CalcTime <= closing_hours)]\n",
    "    \n",
    "    bymnemonic = cleaned_common_stock[['Mnemonic', 'TradedVolume']].groupby(['Mnemonic']).sum()\n",
    "    number_of_stocks = 100\n",
    "    top = bymnemonic.sort_values(['TradedVolume'], ascending=[0]).head(number_of_stocks)\n",
    "    top_k_stocks = list(top.index.values)\n",
    "    cleaned_common_stock = cleaned_common_stock[cleaned_common_stock.Mnemonic.isin(top_k_stocks)]\n",
    "    sorted_by_index = cleaned_common_stock.set_index(['Mnemonic', 'CalcDateTime']).sort_index()\n",
    "    non_empty_days = sorted(list(cleaned_common_stock['Date'].unique()))\n",
    "    new_datetime_index = build_index(non_empty_days, opening_hours_str, closing_hours_str)[\"OrganizedDateTime\"].values\n",
    "    \n",
    "    stocks = []\n",
    "    for stock in top_k_stocks:\n",
    "        stock = basic_stock_features(sorted_by_index, stock, new_datetime_index, inplace=True)\n",
    "        stocks.append(stock)\n",
    "    # prepared should contain the numeric features for all top k stocks,\n",
    "    # for all days in the interval, for which there were trades (that means excluding weekends and holidays)\n",
    "    # for all minutes from 08:00 until 20:00\n",
    "    # in minutes without trades the prices from the last available minute are carried forward\n",
    "    # trades are filled with zero for such minutes\n",
    "    # a new column called HasTrade is introduced to denote the presence of trades\n",
    "    prepared = pd.concat(stocks, axis=0)\n",
    "    prepared.Mnemonic = prepared.Mnemonic.astype('category')\n",
    "    return prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xgb_target (df):\n",
    "    return df.MaxPrice.shift(-1).fillna (method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xgb_features (df, horizon, inplace = False):\n",
    "    n_df = df\n",
    "    if not inplace:\n",
    "        n_df = df.copy ()\n",
    "    \n",
    "    for offset in range(1, horizon+1):\n",
    "        min_price = n_df['MinPrice'].shift (offset).fillna(method='bfill')\n",
    "        max_price = n_df['MaxPrice'].shift (offset).fillna(method='bfill')\n",
    "        start_price = n_df['StartPrice'].shift (offset).fillna(method='bfill')\n",
    "        end_price = n_df['EndPrice'].shift (offset).fillna(method='bfill')\n",
    "        trade_vol = n_df['TradedVolume'].shift (offset).fillna(method='bfill')\n",
    "        num_trades = n_df['NumberOfTrades'].shift (offset).fillna(method='bfill')\n",
    "        \n",
    "        n_df[\"h{}_MinPrice\".format (offset)] = min_price\n",
    "        n_df[\"h{}_MaxPrice\".format (offset)] = max_price\n",
    "        n_df[\"h{}_StartPrice\".format (offset)] = start_price\n",
    "        n_df[\"h{}_EndPrice\".format (offset)] = end_price\n",
    "        n_df[\"h{}_TradeVolume\".format (offset)] = trade_vol\n",
    "        n_df[\"h{}_NumberOfTrades\".format (offset)] = num_trades\n",
    "        \n",
    "    return n_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_date_range (start_date, end_date):\n",
    "    dates = list(pd.date_range(start_date, end_date, freq='D').strftime('%Y-%m-%d'))\n",
    "    data_dir = local_data_folder + '/'\n",
    "    data_subdirs = map(lambda date: data_dir + date, dates)\n",
    "    unprocessed_df = load_csv_dirs(data_subdirs)\n",
    "    print (\"Loaded CSV data set\")\n",
    "    \n",
    "    cleaned_df = clean_data (unprocessed_df, inplace = True)\n",
    "    print (\"Cleaned CSV data set\")\n",
    "    \n",
    "    \n",
    "    xgb_data = create_xgb_features (cleaned_df, 5, inplace=True)\n",
    "    xgb_data['NextMaxPrice'] = create_xgb_target (xgb_data)\n",
    "    print (\"Engineered CSV data set\")\n",
    "    \n",
    "    train_data, validate_data = train_test_split (xgb_data, train_size=0.8, test_size=0.2, shuffle=True)\n",
    "\n",
    "    cols = list(train_data.columns.values)\n",
    "    cols.remove ('NextMaxPrice')\n",
    "    cols = ['NextMaxPrice'] + cols\n",
    "\n",
    "    train_data = pd.get_dummies (train_data[cols])\n",
    "    validate_data = pd.get_dummies (validate_data[cols])\n",
    "    print (\"Data split for training purposes\")\n",
    "    \n",
    "    return (train_data, validate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_dates = list(pd.date_range(from_date, until_date, freq='M').strftime('%Y-%m-%d'))\n",
    "start_dates = list(pd.date_range(from_date, until_date, freq=pd.offsets.MonthBegin(1)).strftime('%Y-%m-%d'))\n",
    "\n",
    "train_data_folder = 'data/train'\n",
    "train_output_folder = train_data_folder +'/train'\n",
    "validate_output_folder = train_data_folder +'/validate'\n",
    "! mkdir -p {train_output_folder}\n",
    "! mkdir -p {validate_output_folder}\n",
    "\n",
    "for i in range(len(start_dates)):\n",
    "    start_date = start_dates[i]\n",
    "    end_date = end_dates[i]\n",
    "    print (\"Reading data for dates {} to {}\".format (start_date, end_date))\n",
    "    train_df, validate_df = engineer_date_range (start_date, end_date)\n",
    "    print (\"Writing CSV data for dates {} to {}\".format (start_date, end_date))\n",
    "    train_df.to_csv(train_output_folder + '/{}-{}.csv'.format (start_date, end_date), header=False, index=False)\n",
    "    validate_df.to_csv(validate_output_folder + '/{}-{}.csv'.format (start_date, end_date), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OR...\n",
    "\n",
    "Run the following sync command to skip having to engineer your own feature set.  After executing the cell below execute the next sync command to upload the data to your own S3 bucket.  If you're savvy with your S3 commands you could also just sync from the source folder to the target folder, cutting out the notebook middleman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 sync 's3://jasbarto-forecast-lab/training/' {train_data_folder}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Stage data to S3 for training\n",
    "\n",
    "Upload the year's worth of training data to S3 in preparation for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_s3_training_uri = 's3:// < YOUR S3 BUCKET > /training/\"\n",
    "\n",
    "! aws s3 sync data/train {aws_s3_training_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a SageMaker training job\n",
    "\n",
    "The code below will programmatically create a training job for the SageMaker XGBoost algorithm.  Alternatively you can create the training job manually via the AWS web console.\n",
    "\n",
    "### To train via the console:\n",
    "\n",
    "Submit your data to the XGBoost algorithm via the Amazon SageMaker web console.  Select `Training Jobs` and click the `Create training job` button.\n",
    "\n",
    "Give the training job a name such as 'xgboost-stock-forecast' or similar.\n",
    "\n",
    "For `Algorithm` select `XGBoost` from the drop down.\n",
    "\n",
    "Accept the defaults for the remainder of the fields and move down to `Hyperparameters`.  Set the values for the Hyperparameters as determined by the earlier hyperparameter tuning job, accepting the default values for all other parameters.\n",
    "\n",
    "For Input data configure two channels.  The first should be given the name 'train' and have the following settings:\n",
    "- `Content-type` -> 'csv'\n",
    "- `Compression type` -> None\n",
    "- `Record wrapper` -> None\n",
    "- `S3 data type` -> S3Prefix\n",
    "- `S3 data distribution type` -> Fully replicated\n",
    "- `S3 location` -> 's3:// < your-bucket-name > / < your-model-prefix > /train'\n",
    "\n",
    "For the second channel give it a name of 'validation' and set its parameters the same as the 'train' channel.  Give the 'validation' channel a different `S3 location` however, setting it to 's3:// < your-bucket-name > / < your-model-prefix > /validation'\n",
    "\n",
    "For the Output data configuration set the `S3 output path` to 's3:// < your-bucket-name > / < your-model-prefix > /output'.\n",
    "\n",
    "### To use the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')\n",
    "\n",
    "job_name = 'xgboost-forecast-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Training job\", job_name)\n",
    "\n",
    "#Ensure that the training and validation data folders generated above are reflected in the \"InputDataConfig\" parameter below.\n",
    "#Ensure the hyperparameter settings are in line with the parameters selected by the tuning job\n",
    "create_training_params = \\\n",
    "{\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": container,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": bucket_path + \"/\" + prefix + \"/output\"\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.m4.4xlarge\",\n",
    "        \"VolumeSizeInGB\": 20\n",
    "    },\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"silent\":\"0\",\n",
    "        \"objective\":\"reg:linear\",\n",
    "        \"num_round\":\"50\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 3600\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": bucket_path + \"/\" + prefix + '/train',\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"csv\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": bucket_path + \"/\" + prefix + '/validate',\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"csv\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "client.create_training_job(**create_training_params)\n",
    "\n",
    "status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print(status)\n",
    "while status !='Completed' and status!='Failed':\n",
    "    time.sleep(60)\n",
    "    status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "    print(status)\n",
    "print (\"Model exported to {}\".format (status['ModelArtifacts']['S3ModelArtifacts']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the model output\n",
    "\n",
    "Let's download and explore the trained model that has been exported to Amazon S3.  The tarball is registed as an attribute of the completed training job.  We can retrieve it, download the tarball and untar it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "client = boto3.client('sagemaker')\n",
    "\n",
    "job_name = ' < YOUR TRAINING JOB NAME > \"\n",
    "job_desc = client.describe_training_job(TrainingJobName=job_name)\n",
    "status = job_desc['TrainingJobStatus']\n",
    "print(status)\n",
    "model_artifact = job_desc['ModelArtifacts']['S3ModelArtifacts']\n",
    "print (\"Model exported to {}\".format (job_desc['ModelArtifacts']['S3ModelArtifacts']))\n",
    "\n",
    "!aws s3 cp {model_artifact} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now untar the exported model and explore its contents\n",
    "## You can do this using '! cmd' notation in the notebook or open a Terminal window from the Jupyter interface\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
